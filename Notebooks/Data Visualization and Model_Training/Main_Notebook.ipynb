{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "01146f2e",
   "metadata": {},
   "source": [
    "# Image Classification with PyTorch & TIMM"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "ed496f44",
   "metadata": {},
   "outputs": [],
   "source": [
    "import torch \n",
    "import torch.nn as nn\n",
    "import torch.optim as optim\n",
    "from torchvision import datasets,models,transforms\n",
    "from torch.utils.data import DataLoader,Dataset\n",
    "import copy \n",
    "import time\n",
    "import pandas as pd\n",
    "import json,os\n",
    "from PIL import Image\n",
    "import webdataset as wds\n",
    "from torch.optim.lr_scheduler import ReduceLROnPlateau\n",
    "from torch import amp\n",
    "from collections import defaultdict\n",
    "import plotly.express as px\n",
    "import cv2\n",
    "import matplotlib.pyplot as plt\n",
    "import numpy as np\n",
    "import timm\n",
    "from sklearn.metrics import f1_score, precision_score, recall_score, balanced_accuracy_score\n",
    "import random\n",
    "from timm.loss import SoftTargetCrossEntropy"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "b8fcedba",
   "metadata": {},
   "source": [
    "## üìÇ Load Training Data from JSON üìù"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "457dc397",
   "metadata": {},
   "outputs": [],
   "source": [
    "with open(r\"C:\\Users\\thaku\\jupyter_notebook _datasets\\Wildlife_dataset\\Dataset\\train_mini.json\",'r') as f:\n",
    "    json_train_data = json.load(f)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "f0965de7",
   "metadata": {},
   "source": [
    "## üîç Inspect Loaded JSON Data üìä"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "de861742",
   "metadata": {},
   "outputs": [],
   "source": [
    "json_train_data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "e8033753",
   "metadata": {},
   "outputs": [],
   "source": [
    "json_train_data[\"images\"]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "5e8f4f70",
   "metadata": {},
   "outputs": [],
   "source": [
    "json_train_data.keys()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "508c08ab",
   "metadata": {},
   "outputs": [],
   "source": [
    "json_train_data[\"categories\"]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "cc86dd83",
   "metadata": {},
   "outputs": [],
   "source": [
    "json_train_data[\"annotations\"]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "f313298b",
   "metadata": {},
   "outputs": [],
   "source": [
    "categories_df = pd.DataFrame(json_train_data[\"categories\"])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "9f164f0a",
   "metadata": {},
   "outputs": [],
   "source": [
    "categories_df"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "b6e559e3",
   "metadata": {},
   "outputs": [],
   "source": [
    "categories_df = categories_df.iloc[:301,:]"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "c507aa51",
   "metadata": {},
   "source": [
    "## üóÇÔ∏è Convert JSON Data to DataFrames üìä"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "7a962b4b",
   "metadata": {},
   "outputs": [],
   "source": [
    "images_df = pd.DataFrame(json_train_data[\"images\"])\n",
    "annotations_df = pd.DataFrame(json_train_data[\"annotations\"])\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "619f89c4",
   "metadata": {},
   "outputs": [],
   "source": [
    "images_df"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "99adcaac",
   "metadata": {},
   "outputs": [],
   "source": [
    "annotations_df"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "bf06a40d",
   "metadata": {},
   "source": [
    "## üîó Merge Categories, Annotations, and Images into Final DataFrame üñºÔ∏è"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "014aac06",
   "metadata": {},
   "outputs": [],
   "source": [
    "category_annotation_df = categories_df.merge(annotations_df,left_on=\"id\",right_on=\"category_id\",suffixes=[\"_cat\",\"_ann\"])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "71284bcb",
   "metadata": {},
   "outputs": [],
   "source": [
    "category_annotation_df.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "097abb27",
   "metadata": {},
   "outputs": [],
   "source": [
    "final_df =category_annotation_df.merge(images_df,left_on=\"image_id\",right_on=\"id\",suffixes=(\"_cat_ann\",\"_img\"))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "9fc5751a",
   "metadata": {},
   "outputs": [],
   "source": [
    "final_df.sample(5)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "7d9d98fe",
   "metadata": {},
   "outputs": [],
   "source": [
    "final_df.columns"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "903c05ab",
   "metadata": {},
   "outputs": [],
   "source": [
    "pd.set_option(\"display.max_columns\", None)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "614bd07b",
   "metadata": {},
   "outputs": [],
   "source": [
    "final_df = final_df.drop(columns=[\"id_cat\",\"date\",\"rights_holder\",\"license\",\"id_ann\",\"id\"])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "21c6865e",
   "metadata": {},
   "outputs": [],
   "source": [
    "final_df.head(5)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "f50f41bb",
   "metadata": {},
   "outputs": [],
   "source": [
    "final_df[\"width\"].median()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "ba0b82f2",
   "metadata": {},
   "outputs": [],
   "source": [
    "final_df[\"common_name\"].unique()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "63a35416",
   "metadata": {},
   "outputs": [],
   "source": [
    "final_df[\"height\"].median()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "d845c437",
   "metadata": {},
   "outputs": [],
   "source": [
    "final_df.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "24e4ccfc",
   "metadata": {},
   "outputs": [],
   "source": [
    "final_df[\"file_name\"][0]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "2ff84d99",
   "metadata": {},
   "outputs": [],
   "source": [
    "new_df_train"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "f447699c",
   "metadata": {},
   "outputs": [],
   "source": [
    "new_df_train=new_df_train.rename(columns={\"file_name\": \"img_path\", \"category_id\": \"class\"})"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "72be1dec",
   "metadata": {},
   "outputs": [],
   "source": [
    "pd.set_option(\"display.max_colwidth\",None)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "de8186fc",
   "metadata": {},
   "outputs": [],
   "source": [
    "new_df_train"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "6c200a60",
   "metadata": {},
   "outputs": [],
   "source": [
    "final_df.describe()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "866ca5c3",
   "metadata": {},
   "source": [
    "## üåç Visualize Species Distribution on World Map üêõüï∑Ô∏è"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "58c6f5b5",
   "metadata": {},
   "outputs": [],
   "source": [
    "species_name =['Common Earthworm', 'Mediterranean Fanworm', 'Serpula columbiana',\n",
    "       'Blue Tube Worm', 'Giant House Spider', 'California Turret Spider',\n",
    "       'Oak Spider', 'Gorse Orbweaver']\n",
    "subset=final_df[final_df[\"common_name\"].isin(species_name)]\n",
    "fig = px.scatter_geo(subset,\n",
    "                     lat=\"latitude\", lon=\"longitude\",\n",
    "                     scope=\"world\",\n",
    "                     title=f\"Locations of {species_name}\",\n",
    "                     opacity=0.7,\n",
    "                     color=\"common_name\")\n",
    "fig.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "69880f27",
   "metadata": {},
   "outputs": [],
   "source": [
    "import plotly.graph_objects as go\n",
    "\n",
    "# Get unique species\n",
    "species_list = final_df[\"common_name\"].unique()\n",
    "\n",
    "# Create empty figure\n",
    "fig = go.Figure()\n",
    "\n",
    "# Add one scatter trace per species\n",
    "for sp in species_list:\n",
    "    subset = final_df[final_df[\"common_name\"] == sp]\n",
    "    fig.add_trace(\n",
    "        go.Scattergeo(\n",
    "            lon=subset[\"longitude\"],\n",
    "            lat=subset[\"latitude\"],\n",
    "            text=subset[\"common_name\"],\n",
    "            mode=\"markers\",\n",
    "            marker=dict(size=6),\n",
    "            name=sp,\n",
    "            visible=False  # initially hidden\n",
    "        )\n",
    "    )\n",
    "\n",
    "# Make the first species visible by default\n",
    "fig.data[0].visible = True\n",
    "\n",
    "# Dropdown menu: one button per species\n",
    "buttons = []\n",
    "for i, sp in enumerate(species_list):\n",
    "    visible = [False] * len(species_list)\n",
    "    visible[i] = True  # only this species visible\n",
    "    buttons.append(\n",
    "        dict(\n",
    "            label=sp,\n",
    "            method=\"update\",\n",
    "            args=[{\"visible\": visible}, {\"title\": f\"Locations of {sp}\"}]\n",
    "        )\n",
    "    )\n",
    "\n",
    "# Add dropdown\n",
    "fig.update_layout(\n",
    "    updatemenus=[dict(active=0, buttons=buttons, x=1.05, y=1.15)],\n",
    "    title=\"Species Distribution Map\",\n",
    "    geo=dict(scope=\"world\")\n",
    ")\n",
    "\n",
    "fig.show()\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "5e1d7789",
   "metadata": {},
   "source": [
    "## üìÇ Load & Visualize Validation Data üåçüêõüï∑Ô∏è"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "df3e99cb",
   "metadata": {},
   "outputs": [],
   "source": [
    "with open(r\"C:\\Users\\thaku\\jupyter_notebook _datasets\\Wildlife_dataset\\Dataset\\val.json\",'r') as f:\n",
    "    json_val_data = json.load(f)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "889e7590",
   "metadata": {},
   "outputs": [],
   "source": [
    "val_categories_df = pd.DataFrame(json_train_data[\"categories\"])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "45c15f6d",
   "metadata": {},
   "outputs": [],
   "source": [
    "val_categories_df"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "ba88f70c",
   "metadata": {},
   "outputs": [],
   "source": [
    "val_categories_df = val_categories_df.iloc[:301,:]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "9e8cdb78",
   "metadata": {},
   "outputs": [],
   "source": [
    "val_images_df = pd.DataFrame(json_val_data[\"images\"])\n",
    "val_annotations_df = pd.DataFrame(json_val_data[\"annotations\"])\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "ac0e5a51",
   "metadata": {},
   "outputs": [],
   "source": [
    "val_category_annotation_df = val_categories_df.merge(val_annotations_df,left_on=\"id\",right_on=\"category_id\",suffixes=[\"_cat\",\"_ann\"])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "e30f7589",
   "metadata": {},
   "outputs": [],
   "source": [
    "val_final_df =val_category_annotation_df.merge(val_images_df,left_on=\"image_id\",right_on=\"id\",suffixes=(\"_cat_ann\",\"_img\"))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "d576dae9",
   "metadata": {},
   "outputs": [],
   "source": [
    "val_final_df = val_final_df.drop(columns=[\"id_cat\",\"date\",\"rights_holder\",\"license\",\"id_ann\",\"id\"])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "3f3f9ce0",
   "metadata": {},
   "outputs": [],
   "source": [
    "val_final_df"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "d4011e26",
   "metadata": {},
   "outputs": [],
   "source": [
    "new_df_val =  val_final_df[[\"file_name\",\"category_id\",\"latitude\",\"longitude\"]]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "bcbe28eb",
   "metadata": {},
   "outputs": [],
   "source": [
    "new_df_val"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "96c8ad9c",
   "metadata": {},
   "outputs": [],
   "source": [
    "new_df_val=new_df_val.rename(columns={\"file_name\": \"img_path\", \"category_id\": \"class\"})\n",
    "new_df_val"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "a2eb85f6",
   "metadata": {},
   "outputs": [],
   "source": [
    "new_df_val.to_csv(\"new_df_val.csv\", index=False)\n",
    "new_df_train.to_csv(\"new_df_train.csv\",index=False)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "384a18c2",
   "metadata": {},
   "outputs": [],
   "source": [
    "import plotly.graph_objects as go\n",
    "\n",
    "# Get unique species\n",
    "species_list = val_final_df[\"common_name\"].unique()\n",
    "\n",
    "# Create empty figure\n",
    "fig = go.Figure()\n",
    "\n",
    "# Add one scatter trace per species\n",
    "for sp in species_list:\n",
    "    subset = val_final_df[val_final_df[\"common_name\"] == sp]\n",
    "    fig.add_trace(\n",
    "        go.Scattergeo(\n",
    "            lon=subset[\"longitude\"],\n",
    "            lat=subset[\"latitude\"],\n",
    "            text=subset[\"common_name\"],\n",
    "            mode=\"markers\",\n",
    "            marker=dict(size=6),\n",
    "            name=sp,\n",
    "            visible=False  # initially hidden\n",
    "        )\n",
    "    )\n",
    "\n",
    "# Make the first species visible by default\n",
    "fig.data[0].visible = True\n",
    "\n",
    "# Dropdown menu: one button per species\n",
    "buttons = []\n",
    "for i, sp in enumerate(species_list):\n",
    "    visible = [False] * len(species_list)\n",
    "    visible[i] = True  # only this species visible\n",
    "    buttons.append(\n",
    "        dict(\n",
    "            label=sp,\n",
    "            method=\"update\",\n",
    "            args=[{\"visible\": visible}, {\"title\": f\"Locations of {sp}\"}]\n",
    "        )\n",
    "    )\n",
    "\n",
    "# Add dropdown\n",
    "fig.update_layout(\n",
    "    updatemenus=[dict(active=0, buttons=buttons, x=1.05, y=1.15)],\n",
    "    title=\"Species Distribution Map\",\n",
    "    geo=dict(scope=\"world\")\n",
    ")\n",
    "\n",
    "fig.show()\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "51a33682",
   "metadata": {},
   "source": [
    "## üé® Probabilistic Background Blur Data Augmentation üñºÔ∏è\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "d85408a5",
   "metadata": {},
   "outputs": [],
   "source": [
    "class ProbBackgroundBlur:\n",
    "    def __init__(self,prob=0.3,min_kernel=7,max_kernel=31):\n",
    "        self.prob=prob\n",
    "        self.min_kernel=min_kernel\n",
    "        self.max_kernel=max_kernel\n",
    "    def __call__(self,img):\n",
    "        if random.random()>self.prob:\n",
    "            return img\n",
    "        if isinstance(img,Image.Image):\n",
    "            img=np.array(img)\n",
    "\n",
    "        kernel_size =random.choice(range(self.min_kernel,self.max_kernel+1,2))\n",
    "        blurred = cv2.GaussianBlur(img,(kernel_size,kernel_size),0)\n",
    "        try:\n",
    "            saliency = cv2.saliency.StaticSaliencySpectralResidual_create()\n",
    "            success,saliencyMap =saliency.computeSaliency(img)\n",
    "            threshold =np.mean(saliencyMap)\n",
    "            mask = (saliencyMap>threshold).astype(np.uint8)\n",
    "            if np.mean(mask)>0.5:\n",
    "                mask=1-mask\n",
    "        except:\n",
    "            gray = cv2.cvtColor(img,cv2.COLOR_RGB2GRAY)\n",
    "            _,mask =cv2.threshold(gray,120,1,cv2.THRESH_BINARY_INV)\n",
    "        mask_3d = np.repeat(mask[:,:,np.newaxis],3,axis=2)\n",
    "        result = np.where(mask_3d==1,img,blurred)\n",
    "        return Image.fromarray(result.astype(np.uint8))"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "7c1ef737",
   "metadata": {},
   "source": [
    "## üîÄ Mixup & CutMix Data Augmentation for Images üñºÔ∏è\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "8d5fae3e",
   "metadata": {},
   "outputs": [],
   "source": [
    "class MixupCutmix:\n",
    "    def __init__(self,mixup_alpha=0.2,cutmix_alpha=1.2,prob=0.8,switch_prob=0.4,num_classes=301):\n",
    "        self.mixup_alpha = mixup_alpha\n",
    "        self.cutmix_alpha = cutmix_alpha\n",
    "        self.prob = prob\n",
    "        self.switch_prob = switch_prob\n",
    "        self.num_classes = num_classes\n",
    "        self.applied = False\n",
    "    def _one_hot(self,label):\n",
    "        return torch.nn.functional.one_hot(label,num_classes=self.num_classes).float()\n",
    "    def _sample_beta(self,alpha):\n",
    "        return np.random.beta(alpha,alpha) if alpha>0 else 1.0\n",
    "    def __call__(self, x,y):\n",
    "        self.applied = False\n",
    "        if np.random.random()>self.prob:\n",
    "            return x,self._one_hot(y)\n",
    "        self.applied = True\n",
    "        B,C,H,W = x.size()\n",
    "        shuffled_idx = torch.randperm(B)    \n",
    "        y_shuffled = self._one_hot(y[shuffled_idx])\n",
    "        if np.random.rand()<self.switch_prob:\n",
    "            lam = self._sample_beta(self.cutmix_alpha)\n",
    "            rx = np.random.randint(W)\n",
    "            ry = np.random.randint(H)\n",
    "            rw = max(int(W*np.sqrt(1-lam)),1)\n",
    "            rh = max(int(H*np.sqrt(1-lam)),1)\n",
    "            x1=  np.clip(rx-rw//2,0,W)\n",
    "            x2 = np.clip(rx + rw // 2, 0, W)\n",
    "            y1 = np.clip(ry - rh // 2, 0, H)\n",
    "            y2 = np.clip(ry + rh // 2, 0, H)\n",
    "            x[:, :, y1:y2, x1:x2] = x[shuffled_idx, :, y1:y2, x1:x2]\n",
    "            lam = 1 - ((x2 - x1) * (y2 - y1) / (W * H))\n",
    "        else:\n",
    "            lam = self._sample_beta(self.mixup_alpha)\n",
    "            x = lam * x + (1 - lam) * x[shuffled_idx]\n",
    "        mixed_y = lam * self._one_hot(y) + (1 - lam) * y_shuffled\n",
    "        return x,mixed_y\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "25f96c4b",
   "metadata": {},
   "source": [
    "## üõ†Ô∏è Define Training and Validation Transform Pipelines ‚ú®\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "89d39208",
   "metadata": {},
   "outputs": [],
   "source": [
    "def get_transform_pipeline(blur_prob=0.2):\n",
    "    transform = transforms.Compose([\n",
    "        transforms.Resize((224,224)),\n",
    "        transforms.RandomHorizontalFlip(),\n",
    "        transforms.RandomVerticalFlip(),\n",
    "        ProbBackgroundBlur(prob=blur_prob,min_kernel=7,max_kernel=31),\n",
    "        transforms.ToTensor(),\n",
    "        transforms.Normalize(mean=[0.485,0.456,0.406],\n",
    "                             std=[0.229,0.224,0.225])\n",
    "    ])\n",
    "    return transform\n",
    "\n",
    "def get_val_transform():\n",
    "    return transforms.Compose([\n",
    "        transforms.Resize((224,224)),\n",
    "        transforms.ToTensor(),\n",
    "        transforms.Normalize(mean=[0.485,0.456,0.406],\n",
    "                             std=[0.229,0.224,0.225])\n",
    "    ])"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "4865825e",
   "metadata": {},
   "source": [
    "## üì¶ Prepare WebDataset & DataLoaders for Training and Validation üñºÔ∏è\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "58541a16",
   "metadata": {},
   "outputs": [],
   "source": [
    "train_transform = get_transform_pipeline(blur_prob=0.2)\n",
    "val_transform = get_val_transform()\n",
    "\n",
    "def has_all_fields(sample):\n",
    "    return all(k in sample for k in [\"jpg\", \"cls\", \"lat\", \"lon\"])\n",
    "\n",
    "train_shards = \"file:C:/Users/thaku/jupyter_notebook _datasets/Wildlife_dataset/shards_train_mini_300/shard-{00000..00003}.tar\"\n",
    "val_shards   = \"file:C:/Users/thaku/jupyter_notebook _datasets/Wildlife_dataset/shards_val_mini_300/shard-00000.tar\"\n",
    "def decode_lat(x): \n",
    "    return torch.tensor(float(x), dtype=torch.float32)\n",
    "\n",
    "def decode_lon(x): \n",
    "    return torch.tensor(float(x), dtype=torch.float32)\n",
    "train_dataset = (\n",
    "    wds.WebDataset(train_shards, handler=wds.warn_and_continue)\n",
    "    .decode(\"pil\")                                  # decode jpg -> PIL\n",
    "    .select(has_all_fields)                         # keep only valid samples\n",
    "    .to_tuple(\"jpg\", \"cls\", \"lat\", \"lon\")           # load all four\n",
    "    .map_tuple(train_transform, int, decode_lat, decode_lon)  # apply transforms + type conversions\n",
    "    .shuffle(1000)\n",
    ")\n",
    "\n",
    "val_dataset = (\n",
    "    wds.WebDataset(val_shards, handler=wds.warn_and_continue)\n",
    "    .decode(\"pil\")                                  # decode jpg -> PIL\n",
    "    .select(has_all_fields)                         # keep only valid samples\n",
    "    .to_tuple(\"jpg\", \"cls\", \"lat\", \"lon\")           # load all four\n",
    "    .map_tuple(val_transform, int, decode_lat, decode_lon)  # apply transforms + type conversions\n",
    ")\n",
    "\n",
    "# -------------------------------\n",
    "# üîπ Step 4: DataLoader\n",
    "# -------------------------------\n",
    "train_dataloader = torch.utils.data.DataLoader(\n",
    "    train_dataset, batch_size=128, num_workers=0\n",
    ")\n",
    "\n",
    "val_dataloader = torch.utils.data.DataLoader(\n",
    "    val_dataset, batch_size=128, num_workers=0, shuffle=False\n",
    ") "
   ]
  },
  {
   "cell_type": "markdown",
   "id": "a1739b5b",
   "metadata": {},
   "source": [
    "## ‚è±Ô∏è Benchmark DataLoader Performance ‚ö°\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "55b76817",
   "metadata": {},
   "outputs": [],
   "source": [
    "import time\n",
    "\n",
    "# warmup\n",
    "for _ in range(2):\n",
    "    for _ in train_dataloader:\n",
    "        break\n",
    "\n",
    "# benchmark\n",
    "start = time.time()\n",
    "for i, (images, labels, lats, lons) in enumerate(train_dataloader):\n",
    "    print(f\"Batch {i} -> {images.shape}, {labels.shape}\")\n",
    "    break  # only load first batch\n",
    "end = time.time()\n",
    "\n",
    "print(f\"Time to load 1 batch = {end - start:.3f} seconds\")\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "93f98d01",
   "metadata": {},
   "source": [
    "## üñºÔ∏èüåç Multi-Modal EfficientNet Model with Image and Location Features üîó\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "75d048f0",
   "metadata": {},
   "outputs": [],
   "source": [
    "class MultiModalEfficientNet(nn.Module):\n",
    "    def __init__(self,num_classes,loc_feat_dim=64,backbone=\"tf_efficientnetv2_s\"):\n",
    "        super().__init__()\n",
    "        self.backbone= timm.create_model(backbone,pretrained=True,num_classes=0)\n",
    "        feat_dim=self.backbone.num_features\n",
    "        for param in self.backbone.parameters():\n",
    "            param.requires_grad=False\n",
    "        \n",
    "        self.loc_mlp=nn.Sequential(\n",
    "            nn.Linear(2,loc_feat_dim),\n",
    "            nn.ReLU(),\n",
    "            nn.Dropout(0.1),\n",
    "            nn.Linear(loc_feat_dim,loc_feat_dim),\n",
    "            nn.ReLU(),\n",
    "            nn.Dropout(0.1),\n",
    "        )\n",
    "\n",
    "        self.classifier = nn.Linear(feat_dim+loc_feat_dim,num_classes)\n",
    "    def forward(self,img,lat,lon):\n",
    "        img_feat=self.backbone(img)\n",
    "        img_feat = nn.functional.dropout(img_feat,p=0.1,training=self.training)\n",
    "        loc_input =torch.stack([lat,lon],dim=1)\n",
    "        loc_feat = self.loc_mlp(loc_input)\n",
    "\n",
    "        fused = torch.cat([img_feat,loc_feat],dim=1)\n",
    "        return self.classifier(fused)\n",
    "    def unfreeze_last_block(self):\n",
    "        last_block = self.backbone.blocks[-1]\n",
    "        for param in last_block.parameters():\n",
    "            param.requires_grad=True"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "5d4b2ea9",
   "metadata": {},
   "source": [
    "## üöÄ Multi-Modal Model Training with Mixed Precision, MixUp/CutMix, and Early Stopping üñºÔ∏èüåç\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "84c5b328",
   "metadata": {},
   "outputs": [],
   "source": [
    "device = torch.device(\"cuda\" if torch.cuda.is_available() else \"cpu\")\n",
    "print(\"Using device:\", device)\n",
    "\n",
    "# ---------------- Model ----------------\n",
    "model = MultiModalEfficientNet(num_classes=301, backbone=\"tf_efficientnetv2_s\").to(device)\n",
    "\n",
    "# ---------------- Checkpoint ----------------\n",
    "checkpoint_path = \"best_checkpoint_multimodal.pth\"\n",
    "\n",
    "start_epoch = 0\n",
    "best_val_loss = float(\"inf\")\n",
    "if os.path.exists(checkpoint_path):\n",
    "    checkpoint = torch.load(checkpoint_path, map_location=device)\n",
    "    model.load_state_dict(checkpoint['model_state_dict'],strict=False)\n",
    "    start_epoch = checkpoint['epoch'] + 1\n",
    "    best_val_loss = checkpoint['best_val_loss']\n",
    "    print(f\"‚úÖ Loaded checkpoint from epoch {checkpoint['epoch']}, val_loss: {best_val_loss:.4f}\")\n",
    "csv_file = \"training_log_3_multimodal.csv\"\n",
    "if os.path.exists(csv_file):\n",
    "    os.remove(csv_file)\n",
    "\n",
    "# Move model to device\n",
    "model = model.to(device)\n",
    "unfrozen = False\n",
    "unfreeze_epoch = 5 \n",
    "# Loss & optimizer\n",
    "criterion = SoftTargetCrossEntropy()\n",
    "optimizer = torch.optim.AdamW(\n",
    "    filter(lambda p: p.requires_grad, model.parameters()),\n",
    "    lr=1e-5, weight_decay=5e-5\n",
    ")\n",
    "\n",
    "scheduler = torch.optim.lr_scheduler.ReduceLROnPlateau(\n",
    "    optimizer, mode=\"min\", factor=0.1, patience=3\n",
    ")\n",
    "\n",
    "# AMP scaler\n",
    "scaler = torch.amp.GradScaler(\"cuda\")\n",
    "\n",
    "# Early stopping\n",
    "# best_val_loss = float(\"inf\")\n",
    "best_model_wts = copy.deepcopy(model.state_dict())\n",
    "patience = 80\n",
    "counter = 0\n",
    "num_epochs = 300\n",
    "\n",
    "# Dict for logging\n",
    "epoc_data = defaultdict(list)\n",
    "mixcut = MixupCutmix(num_classes=301)\n",
    "try:\n",
    "    for epoch in range(start_epoch,num_epochs):\n",
    "        # ---------------- Training ----------------\n",
    "        model.train()\n",
    "        running_loss, correct_top1, correct_top5, total, num_batches = 0.0, 0, 0, 0, 0\n",
    "\n",
    "        for batch_idx, (images, labels, lats, lons) in enumerate(train_dataloader):   # MULTIMODAL BATCH\n",
    "            \n",
    "            images, labels = images.to(device, non_blocking=True), labels.to(device, non_blocking=True)\n",
    "            images, labels = mixcut(images, labels) \n",
    "            lats, lons = lats.to(device, non_blocking=True), lons.to(device, non_blocking=True)\n",
    "            \n",
    "            optimizer.zero_grad()\n",
    "\n",
    "            with torch.autocast(device_type=\"cuda\", dtype=torch.float16):\n",
    "                outputs = model(images, lats, lons)           # MULTIMODAL FORWARD\n",
    "                if labels.dim() == 1:  \n",
    "                    labels = torch.nn.functional.one_hot(labels, num_classes=outputs.size(1)).float()\n",
    "                loss = criterion(outputs, labels)\n",
    "\n",
    "            scaler.scale(loss).backward()\n",
    "            scaler.step(optimizer)\n",
    "            scaler.update()\n",
    "            # ---------------- Debug logging for first batch ----------------\n",
    "            if batch_idx % 10 == 0 or epoch == 0:  # log every 10 batches and first epoch\n",
    "                print(f\"\\nüîé Epoch {epoch+1}, Batch {batch_idx+1}\")\n",
    "                print(\"MixUp/CutMix applied:\", mixcut.applied)\n",
    "                sample_logits = outputs[0].detach().cpu()\n",
    "                sample_probs = nn.functional.softmax(sample_logits, dim=0)\n",
    "                top5_prob, top5_cls = sample_probs.topk(5)\n",
    "                print(\"Top-5 predicted classes:\", top5_cls.tolist())\n",
    "                print(\"Top-5 probabilities:\", top5_prob.tolist())\n",
    "                if labels.dim() > 1:\n",
    "                    print(\"Sample soft labels (first 10 classes):\", labels[0][:10].tolist())\n",
    "                else:\n",
    "                    print(\"Sample hard label:\", labels[0].item())\n",
    "\n",
    "            # Metrics\n",
    "            running_loss += loss.item()\n",
    "            # Convert soft targets to hard labels for accuracy\n",
    "            if labels.dim() > 1:  \n",
    "                true_labels = labels.argmax(dim=1)\n",
    "            else:\n",
    "                true_labels = labels\n",
    "\n",
    "            # Top-1\n",
    "            _, preds = outputs.max(1)\n",
    "            correct_top1 += (preds == true_labels).sum().item()\n",
    "\n",
    "            # Top-5\n",
    "            _, top5_preds = outputs.topk(5, dim=1)\n",
    "            correct_top5 += (top5_preds == true_labels.view(-1, 1)).sum().item()\n",
    "\n",
    "            total += true_labels.size(0)\n",
    "            num_batches += 1\n",
    "\n",
    "\n",
    "\n",
    "        train_loss = running_loss / num_batches\n",
    "        train_acc = 100 * correct_top1 / total\n",
    "        train_top5 = 100 * correct_top5 / total\n",
    "\n",
    "        # ---------------- Validation ----------------\n",
    "        model.eval()\n",
    "        val_loss, correct_top1, correct_top5, total, num_batches = 0.0, 0, 0, 0, 0\n",
    "        all_preds = []\n",
    "        all_true = []\n",
    "\n",
    "        with torch.inference_mode():\n",
    "            for images, labels, lats, lons in val_dataloader:   # MULTIMODAL BATCH\n",
    "                images, labels = images.to(device, non_blocking=True), labels.to(device, non_blocking=True)\n",
    "                lats, lons = lats.to(device, non_blocking=True), lons.to(device, non_blocking=True)\n",
    "        \n",
    "                with torch.autocast(device_type=\"cuda\", dtype=torch.float16):\n",
    "                    outputs = model(images, lats, lons)        # MULTIMODAL FORWARD\n",
    "                    if labels.dim() == 1:\n",
    "                        labels = torch.nn.functional.one_hot(labels, num_classes=outputs.size(1)).float()\n",
    "                    loss = criterion(outputs, labels)\n",
    "\n",
    "                val_loss += loss.item()\n",
    "                if labels.dim() > 1:  \n",
    "                    true_labels = labels.argmax(dim=1)\n",
    "                else:\n",
    "                    true_labels = labels\n",
    "\n",
    "                # Top-1\n",
    "                _, preds = outputs.max(1)\n",
    "                all_preds.extend(preds.cpu().numpy())\n",
    "                all_true.extend(true_labels.cpu().numpy())\n",
    "                correct_top1 += (preds == true_labels).sum().item()\n",
    "\n",
    "                # Top-5\n",
    "                _, top5_preds = outputs.topk(5, dim=1)\n",
    "                correct_top5 += (top5_preds == true_labels.view(-1, 1)).sum().item()\n",
    "\n",
    "                total += true_labels.size(0)\n",
    "                num_batches += 1\n",
    "\n",
    "        val_loss /= num_batches\n",
    "        val_acc = 100 * correct_top1 / total\n",
    "        val_top5 = 100 * correct_top5 / total\n",
    "\n",
    "        # ---------------- Scheduler ----------------\n",
    "        scheduler.step(val_loss)\n",
    "        for param_group in optimizer.param_groups:\n",
    "            print(f\"Current LR: {param_group['lr']}\")\n",
    "\n",
    "        # ---------------- Logging ----------------\n",
    "        f1 = f1_score(all_true, all_preds, average='macro',zero_division=0)\n",
    "        precision = precision_score(all_true, all_preds, average='macro', zero_division=0)\n",
    "        recall = recall_score(all_true, all_preds, average='macro', zero_division=0)\n",
    "\n",
    "# Balanced accuracy\n",
    "        balanced_acc = balanced_accuracy_score(all_true, all_preds)\n",
    "\n",
    "        print(f\"Val F1-score: {f1:.4f}, Precision: {precision:.4f}, Recall: {recall:.4f}, Balanced Acc: {balanced_acc:.4f}\")\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "        print(f\"Epoch [{epoch+1}/{num_epochs}] \"\n",
    "            f\"Train Loss: {train_loss:.4f}, Top-1: {train_acc:.2f}%, Top-5: {train_top5:.2f}% | \"\n",
    "            f\"Val Loss: {val_loss:.4f}, Top-1: {val_acc:.2f}%, Top-5: {val_top5:.2f}%\")\n",
    "\n",
    "        epoc_data[\"Epoch\"].append(epoch+1)\n",
    "        epoc_data[\"Train Loss\"].append(train_loss)\n",
    "        epoc_data[\"Val Loss\"].append(val_loss)\n",
    "        epoc_data[\"Train Top-1 (%)\"].append(train_acc)\n",
    "        epoc_data[\"Val Top-1 (%)\"].append(val_acc)\n",
    "        epoc_data[\"Train Top-5 (%)\"].append(train_top5)\n",
    "        epoc_data[\"Val Top-5 (%)\"].append(val_top5)\n",
    "        epoc_data[\"Val F1-score\"].append(f1)\n",
    "        epoc_data[\"Val Precision\"].append(precision)\n",
    "        epoc_data[\"Val Recall\"].append(recall)\n",
    "        epoc_data[\"Val Balanced Acc\"].append(balanced_acc)\n",
    "        pd.DataFrame(epoc_data).to_csv(csv_file, index=False)\n",
    "\n",
    "        # ---------------- Early stopping ----------------\n",
    "        if val_loss < best_val_loss:\n",
    "            best_val_loss = val_loss\n",
    "            best_model_wts = copy.deepcopy(model.state_dict())\n",
    "            torch.save({\n",
    "                \"epoch\": epoch,\n",
    "                \"model_state_dict\": model.state_dict(),\n",
    "                \"optimizer_state_dict\": optimizer.state_dict(),\n",
    "                \"scheduler_state_dict\": scheduler.state_dict(),\n",
    "                \"best_val_loss\": best_val_loss\n",
    "            }, \"best_checkpoint_multimodal_3.pth\")\n",
    "            print(f\"‚úÖ Model improved & saved at epoch {epoch+1}\")\n",
    "            counter = 0\n",
    "        else:\n",
    "            counter += 1\n",
    "            if counter >= patience:\n",
    "                print(\"‚èπÔ∏è Early stopping triggered\")\n",
    "                break\n",
    "        if not unfrozen and (epoch + 1) >= 94:\n",
    "            print(f\"üîì Unfreezing last MBConv block at epoch {epoch+1}...\")\n",
    "            model.unfreeze_last_block()\n",
    "            optimizer = torch.optim.AdamW(\n",
    "                filter(lambda p: p.requires_grad, model.parameters()), \n",
    "                lr=1e-6, weight_decay=5e-5  # smaller LR for fine-tuning CNN\n",
    "            )\n",
    "            unfrozen = True\n",
    "except KeyboardInterrupt:\n",
    "    print(\"\\n‚èπÔ∏è Training interrupted by user!\")\n",
    "finally:\n",
    "\n",
    "# Load best weights\n",
    "    model.load_state_dict(best_model_wts)\n",
    "    torch.save(model.state_dict(), \"best_model_only_multimodal_3.pth\")"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.9"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
